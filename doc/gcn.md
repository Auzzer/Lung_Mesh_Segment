## 1. Fast adjacency for tetrahedra $\mathcal T_k$

Goal: for each tetra $\mathcal T_k$, build the set of **adjacent tetrahedra**
$$
\mathcal N(k) = \{i_1,\dots,i_{j_k}\}
$$
where each $i_r$ shares a face with $\mathcal T_k$.

### 1.1. Precompute adjacency once (offline)

Since the tetra connectivity doesn’t change for a given mesh, use this pattern:

1. **During preprocessing** (e.g., while running the HU packing script with `tetrahedra` in memory), compute all $\mathcal N(k)$ once.
2. Save it alongside the mesh in the NPZ file (e.g., `tet_neighbors` or an edge list).
3. At training time, load this structure and build the GCN graph instantly.

This alone avoids recomputing adjacency in every training epoch.

### 1.2. Vectorized CPU implementation (highly parallel in C)

This works without explicit Python loops by leveraging NumPy’s vectorized operations (implemented in C and parallelized under the hood):

For a mesh with `M` tets and the tetra–vertex connectivity `tets[k] = (i_1, i_2, i_3, i_4)`:

1) **Generate all faces at once**
* Each tetra has 4 faces:
  $$
  (i_1,i_2,i_3),\ (i_1,i_2,i_4),\ (i_1,i_3,i_4),\ (i_2,i_3,i_4).
  $$
* Build a big array of shape ((4M, 3)) that contains every face of every tet.
* Sort each triple so faces are order‑independent.

2) **Attach tet IDs**

* Build a parallel array of length (4M) that stores, for each face row, the index of the tet it came from:
$$
\text{owner}[f] = k \quad \text{for face } f \text{ generated by } \mathcal T_k.
$$
    
3) **Sort faces globally**

* Lexicographically sort the ((4M,3)) face array.
* Apply the same permutation to `owner`, yielding a sorted list of faces and their owning tets.

4) **Group identical faces**

* Now identical faces (three same vertex indices) are consecutive in the sorted array.
* Scan once through this sorted list:
* If two consecutive entries share the same triple, the corresponding two tet IDs share a face → add them as neighbors of each other.
* Faces that occur only once are boundary faces, and do not produce neighbors.

Because all the heavy work (sorting, grouping) runs in NumPy’s C backend, it is already vectorized and multicore‑friendly. For typical mesh sizes this is **very fast**, and explicit Python‑side parallel loops are unnecessary.



The desired output structure is a list of neighbors per tet:
$$
\mathcal N(k) = \{i_1,\dots,i_{j_k}\}.
$$

---

## 2. Hidden layer: keep **sum** over neighbors
Use this form for the first GCN layer:

$$
\tilde{h}^{(1)}_k
= \sigma\Big(
b^{(1)}+\sum_{j \in \mathcal{N}(k)} \tilde h_j^{(0)} W_{\text{nei}}^{(1)}
+ \tilde h_k^{(0)} W_{\text{self}}^{(1)}
  \Big)
$$

and **do not** average over neighbors. Lock this in, make shapes explicit, then mirror it for the second layer.

### 2.1. Input and hidden features

* Scalar HU per tet: $h_k$.
* Normalized input:
  $$
  \tilde h_k^{(0)} = \frac{h_k - \mu_{\mathrm{HU}}}{\sigma_{\mathrm{HU}}} \in \mathbb{R}.
  $$
* First hidden layer output for each tet:
  $$
  \tilde h_k^{(1)} \in \mathbb{R}^{F_1}.
  $$

### 2.2. First layer (unchanged)

Layer form:

$$
\boxed{
\tilde{h}^{(1)}_k
= \sigma\Big(
b^{(1)}+
\sum_{j \in \mathcal{N}(k)} \tilde h_j^{(0)} W_{\text{nei}}^{(1)}
+ \tilde h_k^{(0)} W_{\text{self}}^{(1)}
  \Big)
  }
$$

with:

* $W_{\text{nei}}^{(1)} \in \mathbb{R}^{1 \times F_1}$: maps each **neighbor scalar** to a hidden vector.
* $W_{\text{self}}^{(1)} \in \mathbb{R}^{1 \times F_1}$: maps the **self scalar** to a hidden vector.
* $b^{(1)} \in \mathbb{R}^{F_1}$: bias.
* $\sigma(\cdot)$: activation (e.g., ReLU) applied component‑wise.

No neighbor averaging — just a **raw sum over neighbors**. Nodes with more neighbors naturally get larger aggregated messages; that is what the formula encodes.

---

## 3. Second layer: same pattern in hidden space

Now repeat the same neighborhood aggregation in the hidden space to get a scalar log‑stiffness per tet.

Let the hidden vectors be $\tilde h^{(1)}_k \in \mathbb{R}^{F_1}$. Define:

* Neighbor weights: $W_{\text{nei}}^{(2)} \in \mathbb{R}^{F_1}$.
* Self weights: $W_{\text{self}}^{(2)} \in \mathbb{R}^{F_1}$.
* Bias: $b^{(2)} \in \mathbb{R}$.

Then:

$$
\boxed{
s_k
= b^{(2)}
+ \sum_{j \in \mathcal N(k)} \langle \tilde h^{(1)}_j, W_{\text{nei}}^{(2)} \rangle
+ \langle \tilde h^{(1)}_k, W_{\text{self}}^{(2)} \rangle
  }
$$

- $s_k \in \mathbb{R}$ is a **raw log‑stiffness** for tetra $k$.
- $\langle \cdot,\cdot \rangle$ is the dot product in $\mathbb{R}^{F_1}$.
- Again: pure **sums over neighbors**, no averaging.

Optionally apply a nonlinearity to $s_k$ (e.g., identity, tanh), but since the next step exponentiates to get $\alpha_k$, it is standard to leave this last map linear.

Finally,

$\alpha_k = \exp \left(\operatorname{clip}\left(s_k, \log \left(\mathrm{ALPHA\_MIN}\right), \log \left(\mathrm{ALPHA\_MAX}\right)\right)\right)$,

so the full 2‑layer model is:

$$
\tilde h_k^{(0)}
\;\xrightarrow{\text{Layer 1 (sum over neighbors)}}
\tilde h_k^{(1)}
\;\xrightarrow{\text{Layer 2 (sum over neighbors)}}
s_k
\;\xrightarrow{\exp,\ \text{clamp}}
\alpha_k.
$$

## 4. Training

### 4.1. Data per case and per time

Train across $C$ CasePacks (e.g., $C=10$: `Case1Pack`, …, `Case10Pack`). For each case $c$:

1) **Geometry and adjacency**

* A single T00 tetrahedral mesh with $M_c$ tetrahedra, defined by:

  * node coordinates $\mathbf x^{(c)} \in \mathbb R^{N_c \times 3}$,
  * tetra connectivity $\mathcal T_k^{(c)} = (v_{i_1}, v_{i_2}, v_{i_3}, v_{i_4})$, $k=1,\dots,M_c$.
* A face‑based tetra adjacency graph $\mathcal N^{(c)}(k)$ built once from `tetrahedra` and stored in the HU pack as
  `tet_neighbor_indices`, `tet_neighbor_offsets` (CSR format). 

2) **Per‑tet HU (one HU field per mesh)**

From the forward HU packing script, we have for each case:

* `hu_tetra_steps`: per‑step HU values sampled on the warped mesh, shape $(S_c, M_c, 1)$,
* `hu_tetra_mean`: time‑average over all steps, shape $(M_c, 1)$.

This work uses **one HU scalar per tetrahedron and per case**, taken from the temporal mean:

$$
h_k^{(c)} := \text{hu\_tetra\_mean}[k, 0], \quad k = 1,\dots,M_c.
$$


This reflects the assumption that tissue density (and hence stiffness) does not change across the breathing cycle for a given lung, while the deformation axes, barycenters, and boundary conditions vary over time.

3) **Per‑timestamp simulation problems**

For each case $c$ and each time step $t\in\{1,\dots,S_c\}$ (e.g. T10→T20, …, T80→T90), the SMS preprocessor produces a preprocessed NPZ that encodes a static equilibrium problem on the **same T00 mesh**:

* `mesh_points`, `tetrahedra`, `volume`, `mass`, boundary node flags, etc.
* `initial_positions` and `displacement_field` for that fixed/moving pair.
* SMS rows `r_axis`, `r_shear`, `r_vol`, capturing the **local deformation axes** for that timestamp.
* An observed displacement of free DOFs $u^{(c,t)}_{\text{obs}}$ derived from registration.

From this NPZ we construct:

* A `ConeStaticEquilibrium` object $\text{sim}_{c,t}$.
* An `SMSLayer_{c,t}` that, given a per‑tet log‑stiffness field, assembles the PDE, solves for $u^{(c,t)}_{\text{sim}}$, and returns the total SMS loss plus components.

Note that **barycenters and deformation axes are updated implicitly at each time step** through `r_axis`, `r_shear`, `r_vol` and the precomputed shape gradients in the SMS preprocessor; the GCN does not need to move the mesh explicitly.

---

### 4.2. GCN forward: HU → log‑stiffness

For each case $c$ form a per‑tet HU vector

$$
\mathbf h^{(c)} = (h_1^{(c)},\dots,h_{M_c}^{(c)}) \in \mathbb R^{M_c},
$$

constant across all timestamps for that case.

Inside the GCN normalize HU per case:

$$
\tilde h_k^{(0,c)} = \frac{h_k^{(c)} - \mu^{(c)}}{\sigma^{(c)} + \varepsilon}, \quad
\mu^{(c)} = \frac{1}{M_c}\sum_{k}h_k^{(c)}, \quad
\sigma^{(c)} = \sqrt{\frac{1}{M_c}\sum_k (h_k^{(c)} - \mu^{(c)})^2}.
$$

Using the adjacency $\mathcal N^{(c)}(k)$, the 2‑layer GCN (Section 3) produces a single **log‑stiffness field** per case, independent of time:

1) **First layer (HU → hidden)**

   $$
   \tilde{h}^{(1,c)}_k
   = \sigma\Big(
   b^{(1)}
   + \sum_{j \in \mathcal N^{(c)}(k)} \tilde h_j^{(0,c)} W_{\text{nei}}^{(1)}
   + \tilde h_k^{(0,c)} W_{\text{self}}^{(1)}
     \Big),
     $$
     where $\tilde h^{(1,c)}_k \in \mathbb R^{F_1}$.
  
2) **Second layer (hidden → log‑α)**

   $$
   s_k^{(c)}
   = b^{(2)}
   + \sum_{j \in \mathcal N^{(c)}(k)} \langle \tilde h_j^{(1,c)}, W_{\text{nei}}^{(2)} \rangle
   + \langle \tilde h_k^{(1,c)}, W_{\text{self}}^{(2)} \rangle,
     $$
     giving a raw log‑stiffness $s_k^{(c)} \in \mathbb R$ per tetra.

3) **Physical box and exponentiation**

Clamp in log‑space to the global physical bounds $[\alpha_{\min}, \alpha_{\max}]$ (500–$10^4$ Pa as in the SMS solver):

$$
\hat s_k^{(c)} = \operatorname{clip}\Bigl(
s_k^{(c)},
\log\alpha_{\min}, \log\alpha_{\max}
\Bigr),
\qquad
\alpha_k^{(c)} = \exp\bigl(\hat s_k^{(c)}\bigr).
$$

In code, `TetGCN` returns $(\alpha^{(c)}, \hat s^{(c)})$ for a given HU vector and adjacency. 

This yields **one stiffness field per case**, shared across all timestamps:

$$
\alpha_k^{(c,t)} \equiv \alpha_k^{(c)}.
$$

A future variant could be generalized to time‑varying stiffness by giving the GCN time‑dependent inputs (e.g., `hu_tetra_steps[t]`, respiratory phase, or a recurrent hidden state). Here the physically motivated assumption of time‑invariant tissue properties is retained.

---

### 4.3. Per‑timestamp simulation with updated deformation

For a given case $c$ and time step $t$:

1) Reuse the same log‑stiffness field $\hat s^{(c)}$ and treat it as the **per‑tet log parameter** field for the SMS layer.

Concretely, we pass:

* $\theta = 0$ (no separate global log parameter),
* $\delta^{(c,t)} = \hat s^{(c)}$ (shape $M_c$),

to the `SMSLayer_{c,t}`:

$$
L_{c,t} =
L\bigl(\theta=0,\ \delta^{(c,t)} = \hat s^{(c)},\ \text{sim}_{c,t},\ u^{(c,t)}_{\text{obs}}\bigr).
$$

Internally, the SMS autograd wrapper computes

$$
\alpha_k^{(c,t)} = \exp\bigl(\operatorname{clip}(\theta + \delta_k^{(c,t)}, \log\alpha_{\min}, \log\alpha_{\max})\bigr),
$$

which matches the clamping and exponentiation applied in the GCN, and assembles the FEM system accordingly. 

2) `sim_{c,t}` uses the precomputed shape gradients, barycenters, and SMS rows `r_axis`, `r_shear`, `r_vol` for that particular fixed–moving pair, so the deformation axes are **different at each timestamp** even though $\alpha$ is shared.

3) The SMS loss combines:
* A data misfit term between $u^{(c,t)}_{\text{sim}}$ and $u^{(c,t)}_{\text{obs}}$.
* A displacement TV regularizer on $\nabla u^{(c,t)}_{\text{sim}}$.
* A log‑TV regularizer on $\log \alpha^{(c)}$, reused at all times. 

`SMSFunction` returns the total scalar loss $L_{c,t}$ plus components and the gradient with respect to the log‑parameters, which is then propagated back through the GCN.

---

### 4.4. Case‑level loss and optimizer updates

For each case $c$ there is a sequence of timestamps $t=1,\dots,S_c$ (e.g. T10→T20,…,T80→T90). Since the stiffness field $\alpha^{(c)}$ is assumed constant across time, all these timestamps jointly constrain the same parameters.

Define the **case‑level loss** as the average over timestamps:

$$
L_c(\Theta) =
\frac{1}{S_c}\sum_{t=1}^{S_c} L_{c,t}(\Theta),
$$

where $\Theta$ are all GCN parameters.

Training proceeds by looping over cases and aggregating per‑timestamp losses before each optimizer update:

1. Choose a case $c$.
2. Compute HU vector $\mathbf h^{(c)}$ once (from `hu_tetra_mean[:,0]`) and run the GCN once to get $\hat s^{(c)}$ and $\alpha^{(c)}$.
3. For each timestamp $t$ of this case:

   * Call the corresponding `SMSLayer_{c,t}(theta=0, delta=\hat s^{(c)})`.
   * Accumulate $L_{c,t}$ into an average $L_c$.
4. Backpropagate $L_c$ and perform one optimizer step on $\Theta$.

This “one update per case” strategy:

* Uses all timestamps for that case as a **mini‑batch**,
* Produces more stable gradients than updating after every single timestamp,
* Matches the physical assumption that one stiffness field must explain the whole breathing cycle for that subject.

The global training objective across all cases is

$$
\mathcal L(\Theta) =
\frac{1}{C} \sum_{c=1}^{C} L_c(\Theta).
$$

In practice we iterate over cases in random order for each epoch and use a standard optimizer (here we use Adam) to update the GCN parameters.

---

### 4.5. Learnable linear HU → log‑stiffness mapping

Our per‑tetra HU values (from `hu_tetra_mean`) lie roughly in
[-972.63, 202.33] Hounsfield units. 
These values are not stiffness themselves, but it is reasonable to map
HU monotonically to stiffness, as in prior lung biomechanics work
(e.g., Shiinoki et al. 2023, who use HU‑dependent stress maps).
Rather than fixing this mapping a priori, we introduce a small
**learnable linear layer** from HU to log‑stiffness and let the GCN
learn a spatial residual on top.

For each case $c$ and tetrahedron $k$ define a baseline
log‑stiffness

$$
s^{\mathrm{lin}}_k := a + b\,h_k^{(c)},
$$

where $h_k^{(c)}$ is the (time‑averaged) HU of that tetra,
and $a, b \in \mathbb{R}$ are global trainable parameters shared
across all tets and cases. This layer captures a simple, monotone
HU→log‑stiffness relationship analogous to the linear HU–stress laws
used in previous work, but with coefficients adapted from data rather
than fixed.

The graph convolutional part then learns a residual correction based on
the spatial HU pattern and tetrahedral adjacency. Let

$$
r_k^{(c)} = \text{GCN}(h^{(c)}, \mathcal N^{(c)})_k
$$

denote the output of the two‑layer GCN described in Section 3
(sum over neighbors in both layers).
We combine the baseline and residual as

$$
s_k^{(c)} = s^{\mathrm{lin}}_k + r_k^{(c)}.
$$

To ensure physical plausibility clamp in log‑space to the global
stiffness bounds $[\alpha_{\min},\alpha_{\max}]$, matching the box
used in the SMS solver:

$$
\hat s_k^{(c)} =
\operatorname{clip}\bigl(
  s_k^{(c)},\,\log\alpha_{\min},\,\log\alpha_{\max}
\bigr),
\qquad
\alpha_k^{(c)} = \exp\bigl(\hat s_k^{(c)}\bigr).
$$

The SMS layer receives $\hat s^{(c)}$ as its per‑tet log‑parameter
field and uses it to assemble the PDE and compute the SMS loss for each
timestamp.
Gradients of this loss propagate back through both the linear layer
$(a,b)$ and the GCN parameters.

#### Initialization of the linear mapping

We initialize \(a\) and \(b\) so that, at the start of training, the
linear mapping already produces log‑stiffness values spanning the
physical range for the typical HU range observed in the data.

Let $h_{\min}$ and $h_{\max}$ be the global minimum and maximum of
$\texttt{hu\_tetra\_mean}$ over the training set
(e.g., $h_{\min}\approx -973$, $h_{\max}\approx 202$),
and let

$$
L_{\min} = \log\alpha_{\min},\qquad
L_{\max} = \log\alpha_{\max}
$$

be the log‑bounds used by the SMS solver (e.g., 500 and $10^4$ Pa).
Choose the initial slope and intercept such that

$$
s^{\mathrm{lin}}(h_{\min}) \approx L_{\min},\qquad
s^{\mathrm{lin}}(h_{\max}) \approx L_{\max},
$$

i.e.,

$$
b^{(0)} = \frac{L_{\max} - L_{\min}}{h_{\max} - h_{\min}},
\qquad
a^{(0)} = L_{\min} - b^{(0)} h_{\min}.
$$

With this initialization, a tetra with HU near the lung‑air range
maps to log‑stiffness near $L_{\min}$, and a tetra with HU near the
denser range maps to log‑stiffness near $L_{\max}$, before any graph
corrections. The GCN residual $r_k^{(c)}$ is initialized with small
weights and zero biases, so that $r_k^{(c)} \approx 0$ at the start,
and

$$
s_k^{(c)} \approx s^{\mathrm{lin}}_k.
$$

During training, both the linear coefficients $(a,b)$ and the GCN
parameters are updated jointly under the SMS loss. The linear layer thus
captures a global HU→stiffness trend, while the graph convolution
learns spatially varying corrections that improve agreement with the
patient‑specific 4DCT displacements.


### 4.6. On possible time‑varying stiffness

The current design enforces $\alpha_k^{(c,t)} \equiv \alpha_k^{(c)}$ for all timestamps $t$ of a case:

* This matches the physical intuition that parenchymal stiffness is a property of the tissue, not of the breathing phase.
* With only ten cases, allowing a separate stiffness field at each timestamp would be severely under‑constrained.

If a time‑dependent stiffness model is needed later, the framework can be extended in two directions:

1. Use **time‑dependent HU** (`hu_tetra_steps[t,k,0]`) or other per‑step features (respiratory phase, barycenter displacement norms) as GCN inputs.
2. Introduce a true **graph‑convolutional RNN**, where the hidden state $z_k^{(c,t)}$ depends on both $z_k^{(c,t-1)}$ and the current HU signal, so that $\alpha_k^{(c,t)}$ can evolve over time.

The training scheme would remain the same—sum losses over timestamps per case and backpropagate through the unrolled sequence—but this work focuses on the simpler and better‑posed time‑invariant stiffness model.
